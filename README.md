# üìä Monitoramento de CPU: Dashboard Interativo com Streamlit

## Vis√£o Geral do Projeto

Este projeto implementa um pipeline **ETL (Extract, Transform, Load)** completo para monitorar dados de telemetria da CPU (dados extra√≠dos via log do [Coretemp](https://www.alcpu.com/CoreTemp/)
 ), incluindo **temperatura dos n√∫cleos**, **velocidade de clock** e **consumo de energia**. Os dados s√£o coletados, limpos, transformados, e carregados em um banco de dados **PostgreSQL**. Posteriormente, um dashboard interativo √© desenvolvido utilizando **Streamlit** para visualizar e analisar essas m√©tricas importantes da CPU. O objetivo √© fornecer uma ferramenta para acompanhar o desempenho e o comportamento t√©rmico e energ√©tico do processador.

## Funcionalidades Principais

*   **Processamento de Dados Automatizado**: Um script Python (`pipeline.py`) para automatizar a limpeza e organiza√ß√£o de arquivos CSV brutos.
*   **Armazenamento de Dados Robustos**: Carregamento seguro dos dados processados em um banco de dados **PostgreSQL**, com tratamento de transa√ß√µes para garantir a integridade dos dados. (Como a pipeline produz arquivos csv, os arquivos csv podem ser diretamente conectados ao dashboard, evitando o uso de banco de dados)
*   **Dashboard Interativo com Streamlit**: Uma aplica√ß√£o web (`app.py`) que oferece as seguintes visualiza√ß√µes din√¢micas:
    *   **Temperatura do N√∫cleo vs. Velocidade do N√∫cleo**
    *   **Temperatura do N√∫cleo ao Longo do Dia**
    *   **Consumo de Energia da CPU ao Longo do Dia**
    *   **Consumo de Energia da CPU vs. Temperatura do N√∫cleo**

## Estrutura do Projeto

A organiza√ß√£o do projeto segue a seguinte estrutura de arquivos:

Registros CPU
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ charts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ line_charts.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ queries/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ queries.py 
‚îÇ   ‚îî‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ data_loaded_processed/
‚îú‚îÄ‚îÄ data_loaded_raw/
‚îú‚îÄ‚îÄ data_processed/
‚îú‚îÄ‚îÄ data_raw/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ exploracao.ipynb
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ load.py
‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py
‚îú‚îÄ‚îÄ amostra_dados_brutos.xlsx
‚îî‚îÄ‚îÄ README.md

## Tecnologias Utilizadas

O projeto faz uso das seguintes tecnologias e bibliotecas:

*   **Python**: A linguagem de programa√ß√£o central para todas as etapas do projeto.
*   **Pandas**: Utilizada para manipula√ß√£o e an√°lise eficiente de DataFrames durante o processamento e leitura de dados.
*   **Streamlit**: Framework para a constru√ß√£o r√°pida de aplica√ß√µes web interativas e o dashboard.
*   **Altair**: Biblioteca de visualiza√ß√£o usada para criar os gr√°ficos interativos no dashboard.
*   **SQLAlchemy**: Toolkit Python SQL para intera√ß√£o com o banco de dados PostgreSQL.
*   **Psycopg2**: Adaptador PostgreSQL para Python, utilizado pela SQLAlchemy para conex√£o com o banco de dados.
*   **PostgreSQL**: Sistema de gerenciamento de banco de dados relacional (SGBD) utilizado para armazenar os dados de telemetria da CPU.

## Dados

Os dados de entrada s√£o arquivos CSV contendo telemetria da CPU. As colunas principais incluem:

*   `time`: Data/hora da coleta.
*   `core_temp_X`: Temperatura atual de cada n√∫cleo (0 a 5) em graus Celsius (¬∞C).
*   `low_temp_X` / `high_temp_X`: Temperaturas m√≠nimas e m√°ximas registradas para cada n√∫cleo.
*   `core_load_X`: Carga de trabalho de cada n√∫cleo (em %).
*   `core_speed_X`: Velocidade de clock de cada n√∫cleo (em MHz).
*   `cpu_power`: Consumo total de energia da CPU.

## Processo ETL Detalhado

O processo ETL √© dividido em duas etapas principais, executadas pelos scripts `pipeline.py` (Transforma√ß√£o) e `load.py` (Carregamento).

### Transforma√ß√£o (`pipeline.py`)

O script `pipeline.py` √© respons√°vel por preparar os dados brutos para o armazenamento e an√°lise.

1.  **Leitura de Arquivos**: L√™ arquivos CSV da pasta `data_raw`, ignorando as 7 primeiras linhas e utilizando a codifica√ß√£o `latin1` para garantir a compatibilidade.
2.  **Limpeza de Colunas**:
    *   Remove colunas que come√ßam com o prefixo "Unnamed".
    *   Descarta colunas de resumo gen√©ricas como "Core 0", "Core 1", etc.
    *   Elimina quaisquer colunas que estejam completamente vazias.
3.  **Normaliza√ß√£o de Tempo**: A coluna `Time` √© convertida para o formato `datetime` (`"%H:%M:%S %m/%d/%y"`), corrigindo poss√≠veis erros.
4.  **Remo√ß√£o de Valores Nulos**: Linhas com valores completamente vazios ou com qualquer valor nulo s√£o removidas para garantir a qualidade dos dados.
5.  **Reordena√ß√£o e Renomea√ß√£o**: As colunas s√£o selecionadas, reordenadas e renomeadas para um padr√£o `snake_case` para padroniza√ß√£o e facilidade de consulta no banco de dados.
6.  **Salvamento e Movimenta√ß√£o**: O DataFrame processado √© salvo como um novo arquivo CSV na pasta `data_processed`. O arquivo original de `data_raw` √© ent√£o movido para `data_loaded_raw`, indicando que foi processado com sucesso.

### üì• Carregamento (`load.py`)

O script `load.py` √© respons√°vel por carregar os dados transformados no banco de dados PostgreSQL.

1.  **Configura√ß√£o de Conex√£o**: Conecta-se a um banco de dados PostgreSQL no meu caso: esquema `coretemp` e na tabela `raw_data`.
2.  **Processamento por Arquivo**: Itera sobre os arquivos CSV localizados na pasta `data_processed`.
3.  **Carregamento Transacional**: Cada arquivo √© lido para um DataFrame Pandas e seus dados s√£o adicionados √† tabela `coretemp.raw_data` usando `append`. O carregamento √© feito dentro de uma transa√ß√£o, o que significa que se um erro ocorrer durante o processamento de *qualquer* arquivo, toda a transa√ß√£o √© revertida (`rollback`), garantindo que nenhum dado parcial seja gravado.
4.  **Movimenta√ß√£o Condicional**: Apenas se o carregamento do arquivo for bem-sucedido e a transa√ß√£o for confirmada (`commit`), o arquivo √© movido da pasta `data_processed` para `data_loaded_processed`.

## Dashboard Interativo (`app.py`)

O dashboard Streamlit exibe quatro gr√°ficos de linha, utilizando dados consultados do PostgreSQL. Todos os gr√°ficos utilizam a fun√ß√£o `grafico_linhas` (definida em `src/charts/line_charts.py`) que renderiza gr√°ficos **Altair** com pontos, eixos `x` e `y` com t√≠tulos, e cores para diferenciar os tipos de agrega√ß√£o (M√≠nimo, M√©dio, M√°ximo).

### Gr√°ficos Exibidos:

1.  **Temperatura do N√∫cleo vs. Velocidade do N√∫cleo**:
    *   **T√≠tulo**: 'Temperatura do N√∫cleo vs Velocidade do N√∫cleo'
    *   **Eixo X**: "core temp"
    *   **Eixo Y**: "core speed"
    *   Exibe os valores M√≠nimo, M√©dio e M√°ximo da velocidade do n√∫cleo para cada temperatura.
2.  **Temperatura do N√∫cleo ao Longo do Dia**:
    *   **T√≠tulo**: 'Temperatura do N√∫cleo ao Longo do Dia'
    *   **Eixo X**: "time of day" (hora do dia)
    *   **Eixo Y**: "core temp"
    *   Mostra os valores M√≠nimo, M√©dio e M√°ximo da temperatura do n√∫cleo para cada hora do dia.
3.  **Energia do CPU ao Longo do Dia**:
    *   **T√≠tulo**: 'Energia do CPU ao Longo do Dia'
    *   **Eixo X**: "time of day" (hora do dia)
    *   **Eixo Y**: "cpu power"
    *   Apresenta os valores M√≠nimo, M√©dio e M√°ximo do consumo de energia da CPU para cada hora do dia.
4.  **Energia do CPU vs. Temperatura do N√∫cleo**:
    *   **T√≠tulo**: 'Energia do CPU vs Temperatura do N√∫cleo'
    *   **Eixo X**: "core temp"
    *   **Eixo Y**: "cpu power"
    *   Ilustra os valores M√≠nimo, M√©dio e M√°ximo do consumo de energia da CPU para cada temperatura do n√∫cleo.

## Como Executar o Projeto

Para configurar e executar o projeto, siga os passos abaixo:

1.  **Configura√ß√£o do Ambiente Python**:
    *   Recomendo criar e ativar um ambiente virtual:
        python -m venv venv
        # Para Windows:
        .\venv\Scripts\activate
        # Para macOS/Linux:
        source venv/bin/activate

    *   Instale as depend√™ncias necess√°rias:
        `pandas streamlit altair sqlalchemy psycopg2-binary`

2.  **Configura√ß√£o do PostgreSQL**:
    *   Certifique-se de ter uma inst√¢ncia do PostgreSQL (o projeto espera `localhost:5432`).
    *   Crie seu banco de dados.
    *   Ajuste as credenciais nos arquivos `load.py` e `queries.py` conforme sua configura√ß√£o.

3.  **Execute o arquivo run_main.bat**: isso inicializar√° o arquivo main.py, criando os seguintes diret√≥rios:
    *   `data_raw`
    *   `data_loaded_raw`
    *   `data_processed`
    *   `data_loaded_processed`
4.  **Dados Brutos**: Coloque seus arquivos CSV de telemetria da CPU (extra√≠dos do `coretemp`) dentro da pasta `data_raw`.

5.  **Executar o Pipeline ETL**:
    *   Volte ao terminal onde est√° sendo executado o main.py e siga as instru√ß√µes para processar os arquivos:
        python pipeline.py
        Este script mover√° os arquivos de `data_raw` para `data_loaded_raw` e salvar√° os arquivos processados em `data_processed`.
    *   Em seguida, continue seguindo as instru√ß√µes no terminal para inserir os dados no PostgreSQL:
        python load.py
        Este script carregar√° os arquivos de `data_processed` para a tabela `raw_data` no esquema `coretemp` e mover√° os arquivos para `data_loaded_processed`.
        `Voc√™ pode modificar os parametros para upload dos arquivos para onde desejar dentro do arquivo load.py.`

6.  **Executar o Dashboard Streamlit**:
       `Os par√¢metros para cria√ß√£o do dashboard est√£o no arquivo app.py dentro do diret√≥rio dashboard.`
    *   Inicie a aplica√ß√£o Streamlit:
        Execute o arquivo run_dashboard.bat
    *   O dashboard ser√° automaticamente aberto em seu navegador web padr√£o.

## Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas!